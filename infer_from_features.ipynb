{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, yaml, sys, torch, random, math, pdb, librosa\n",
    "from torch.backends import cudnn\n",
    "import soundfile as sf\n",
    "from synthesis import build_model\n",
    "from synthesis import wavegen\n",
    "import numpy as np\n",
    "import utils\n",
    "import pyworld as pw\n",
    "from model_vc import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73156685, 0.9653258 , 0.26951085, 0.55747944, 0.66474557],\n",
       "       [0.76360934, 0.04301652, 0.84744789, 0.87270876, 0.63222921],\n",
       "       [0.74134197, 0.41038161, 0.062632  , 0.41055347, 0.82930991],\n",
       "       [0.4379049 , 0.86622844, 0.92990204, 0.35953442, 0.14476378]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72797804, 0.02055924, 0.3429797 , 0.4003369 , 0.01792208],\n",
       "       [0.02140964, 0.44392969, 0.2269577 , 0.59356027, 0.88909418],\n",
       "       [0.63933121, 0.09737466, 0.56183569, 0.09356484, 0.0765063 ],\n",
       "       [0.20519994, 0.96881978, 0.74592682, 0.6392162 , 0.46917686],\n",
       "       [0.79349126, 0.04153629, 0.83147528, 0.19702562, 0.8672483 ],\n",
       "       [0.09191803, 0.08915284, 0.82390163, 0.71087521, 0.68177393],\n",
       "       [0.95905296, 0.11097443, 0.97060312, 0.15213986, 0.89463115],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix_feat_length(feats, crop_size):\n",
    "    random.seed(1)\n",
    "    if feats.shape[0] > crop_size:\n",
    "        diff = feats.shape[0] - crop_size\n",
    "        offset = random.randint(0, diff)\n",
    "        random_offsets.append(offset)\n",
    "        feats = feats[offset:offset+crop_size]\n",
    "    else:\n",
    "        diff = crop_size - feats.shape[0]\n",
    "        if diff % 2 == 0:\n",
    "            feats = np.pad(feats, pad_width=((diff//2, diff//2),(0,0)), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            feats = np.pad(feats, pad_width=((diff//2, diff//2+1),(0,0)), mode='constant', constant_values=0)\n",
    "    return feats\n",
    "\n",
    "qwe = np.random.rand(7,5)\n",
    "asd = fix_feat_length(qwe, 8)\n",
    "asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_library_path = '/homes/bdoc3/my_utils'\n",
    "sys.path.insert(1, my_library_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.worldvocoder import chandna_feats, mfsc_to_sp, midi_to_worldf0, mfsc_to_world_to_audio\n",
    "from utils import setup_sie, setup_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser data\n",
    "svc_model_name = 'withF0chandna_to_500_unnormed'\n",
    "svc_model_dir = '/homes/bdoc3/my_data/autovc_models/autoSvc'\n",
    "svc_ckpt_iters = 252500\n",
    "which_cuda = 1\n",
    "feature_dir = '/homes/bdoc3/my_data/world_vocoder_data/damp_inton/withF0chandna_to_500_unnormed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine relevant paths and yaml files based on user input\n",
    "autovc_model_dir = os.path.join(svc_model_dir, svc_model_name)\n",
    "autosvc_config = pickle.load(open(autovc_model_dir +'/config.pkl','rb'))\n",
    "autosvc_config.autovc_ckpt = os.path.join(autovc_model_dir, 'ckpts', f'ckpt_{svc_ckpt_iters}.pth.tar')\n",
    "with open(feature_dir +'/feat_params.yaml') as File:\n",
    "    feat_params = yaml.load(File, Loader=yaml.FullLoader)\n",
    "subdir_for_wavs = os.path.join(autovc_model_dir, 'generated_wavs')\n",
    "if os.path.exists(subdir_for_wavs)==False:\n",
    "            os.makedirs(subdir_for_wavs)\n",
    "\n",
    "# determine other derivative variables\n",
    "autosvc_config.which_cuda = which_cuda\n",
    "autosvc_config.device = torch.device(f'cuda:{autosvc_config.which_cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/homes/bdoc3/my_data/world_vocoder_data/damp_inton/withF0chandna_to_500_unnormed/val/1198871647/1198871647_1859856826.npy', '/homes/bdoc3/my_data/world_vocoder_data/damp_inton/withF0chandna_to_500_unnormed/val/575984180/575984180_1660276054.npy', '/homes/bdoc3/my_data/world_vocoder_data/damp_inton/withF0chandna_to_500_unnormed/val/1224937687/1224937687_1849548794.npy', '/homes/bdoc3/my_data/world_vocoder_data/damp_inton/withF0chandna_to_500_unnormed/val/1427897033/1427897033_1721850720.npy']\n"
     ]
    }
   ],
   "source": [
    "file_path = '/import/c4dm-datasets/NUS_corpus/MPOL/sing/19.wav'\n",
    "\n",
    "feats_ids = ['1014969323_1909869121', '1026003246_1660051761'] # female alto (low) and falsetto tenor (high)\n",
    "dataset_root = '/import/c4dm-datasets/DAMP_Intonation_Dataset/vocal_tracks'\n",
    "file_ext = '.m4a'\n",
    "\n",
    "feats_paths = [os.path.join(feature_dir, 'val', a_id.split('_')[0], a_id)+'.npy' for a_id in feats_ids]\n",
    "\n",
    "user_input = '/homes/bdoc3/my_data/world_vocoder_data/damp_inton/withF0chandna_to_500_unnormed/val'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "from my_os import recursive_file_retrieval\n",
    "\n",
    "# if input is not file type\n",
    "if os.path.isdir(user_input):\n",
    "    subdir_list, file_path_list = recursive_file_retrieval(user_input)\n",
    "    selected_file_paths = random.choices(file_path_list, k=4)\n",
    "    user_input = selected_file_paths\n",
    "\n",
    "# if type(user_input) is not list:\n",
    "#     user_input = [user_input]\n",
    "\n",
    "feats_paths = user_input\n",
    "print(feats_paths)\n",
    "    \n",
    "#if file is numpy:\n",
    "    #get random clip\n",
    "#else\n",
    "    #get features\n",
    "    # select random clip\n",
    "    # synthesize and save random clip\n",
    "\n",
    "\n",
    "\n",
    "# if dir, then select random examples from it\n",
    "\n",
    "#if file is numpy do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(1, autovc_model_dir)\n",
    "# from this_model_vc import Generator\n",
    "\n",
    "G = setup_gen(autosvc_config, Generator, feat_params['num_feats'])\n",
    "sie = setup_sie(autosvc_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_offsets = []\n",
    "autosvc_config.len_crop = 160\n",
    "\n",
    "def trim_feat_length(feats, cropped_size):\n",
    "    random.seed(1)\n",
    "    if feats.shape[0] > cropped_size:\n",
    "        diff = feats.shape[0] - cropped_size\n",
    "        offset = random.randint(0, diff)\n",
    "        random_offsets.append(offset)\n",
    "        feats = feats[offset:offset+cropped_size]\n",
    "    else:\n",
    "        diff = cropped_size - feats.shape[0]\n",
    "        if diff % 2 != 0:\n",
    "            diff += 1\n",
    "            feats = feats[:-1]\n",
    "        feats = np.pad(feats, pad_width=((diff/2, diff/2),(0,0)), mode='constant', constant_values=0)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def infer(feats_paths, si_encoder, autosvc_G):\n",
    "    \"feeds features into framework and outputs list containing outputs and paths\"\n",
    "    feats_list = [torch.from_numpy(np.load(feat_path)).to(autosvc_config.device).float() for feat_path in feats_paths] # convert to torch objects\n",
    "    outputs_sources_list = []\n",
    "    for trg_path, trg_feats in zip(feats_paths, feats_list):\n",
    "        feats_numpy = trg_feats.cpu().detach().numpy()\n",
    "        synthed_audio = mfsc_to_world_to_audio(feats_numpy[:,:feat_params['num_feats']-4], feats_numpy[:,feat_params['num_feats']-4:-2], feats_numpy[:,-2:], feat_params)\n",
    "        sf.write(subdir_for_wavs +f'/{os.path.basename(trg_path)[:-4]}.wav', synthed_audio, samplerate=feat_params['sr'])\n",
    "        \n",
    "        for src_path, src_feats in zip(feats_paths, feats_list):\n",
    "            \n",
    "#             trimmed_feats_list = [trim_feat_length(feats, autosvc_config.len_crop) for feats in [trg_feats, src_feats]]\n",
    "#             spec_env_list = [trimmed_feats[:,:feat_params['num_feats']] for trimmed_feats in trimmed_feats_list]\n",
    "            trimmed_trg_feats = trim_feat_length(trg_feats, autosvc_config.len_crop)\n",
    "            trimmed_src_feats = trim_feat_length(src_feats, autosvc_config.len_crop)\n",
    "            spec_env_trg = trimmed_trg_feats[:,:feat_params['num_feats']]\n",
    "            spec_env_src = trimmed_src_feats[:,:feat_params['num_feats']]\n",
    "            src_pitch_feats = trimmed_src_feats[:,feat_params['num_feats']:]\n",
    "            src_pitch_feats_np = src_pitch_feats.cpu().detach().numpy()\n",
    "            si_emb, _ = si_encoder(spec_env_trg.unsqueeze(0))\n",
    "            _, psnt_output, _, _, _ = autosvc_G(spec_env_src.unsqueeze(0), si_emb, si_emb)\n",
    "            psnt_output_np = psnt_output.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
    "#             print('inner', trg_path, src_path)\n",
    "            outputs_sources_list.append((psnt_output_np, src_pitch_feats_np, trg_path, src_path))\n",
    "            \n",
    "        trimmed_trg_feats = trim_feat_length(trg_feats, autosvc_config.len_crop)\n",
    "        trimmed_trg_feats_np = trimmed_trg_feats.cpu().detach().numpy()\n",
    "        trg_pitch_feats_np = trimmed_trg_feats_np[:,feat_params['num_feats']:]\n",
    "#         print('outer', trg_path)\n",
    "        outputs_sources_list.append((trimmed_trg_feats_np, trg_pitch_feats_np, 'path/to/original.npy', trg_path))\n",
    "        \n",
    "    return outputs_sources_list\n",
    "\n",
    "outputs_sources_list = infer(feats_paths, sie, G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "for i, (feats, src_pitch_feats, trg_path, src_path) in enumerate(outputs_sources_list):\n",
    "    print(i)\n",
    "    harm_mfsc, ap_mfsc = feats[:,:40], feats[:,40:44]\n",
    "    midi_voicings = src_pitch_feats\n",
    "#     pdb.set_trace()\n",
    "    synthed_audio = mfsc_to_world_to_audio(harm_mfsc, ap_mfsc, midi_voicings, feat_params)\n",
    "    trg_singer, src_singer = os.path.basename(trg_path)[:-4], os.path.basename(src_path)[:-4]\n",
    "    sf.write(subdir_for_wavs +f'/-source-{src_singer}-target-{trg_singer}.wav', synthed_audio, samplerate=feat_params['sr'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet was trained on 80 mel spectrogram. not suitable for 40 mel spectral envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_synth = build_model().to(autosvc_config.device)\n",
    "voice_synth.load_state_dict(synth_voice_ckpt[\"state_dict\"])\n",
    "voice_synth.to(autosvc_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results = pickle.load(open('results.pkl', 'rb'))\n",
    "# subdir_for_conversion = subdir_for_wavs +'/conversions'\n",
    "# if os.path.exists(subdir_for_conversion)==False:\n",
    "#     os.makedirs(subdir_for_conversion)\n",
    "result = results[0]\n",
    "name = result[0]\n",
    "c = result[1]\n",
    "print(name, c[:].shape, type(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "waveform = wavegen(voice_synth, 1, c=c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "#     librosa.output.write_wav(name+'.wav', waveform, sr=16000)\n",
    "#     sf.write(subdir_for_conversion +'/' +name +'.wav', waveform, samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "for i, (feats, na, nb) in enumerate(outputs_sources_list):\n",
    "    feats_reshaped = feats.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
    "    pdb.set_trace()\n",
    "    waveform = wavegen(voice_synth, 1, c=feats_reshaped)\n",
    "#     waveform = wavegen(voice_synth, autosvc_config.which_cuda, c=feats_reshaped)\n",
    "    sf.write(subdir_for_wavs +f'/sourceId_{na}_to_targetId_{nb}.wav', waveform, samplerate=feat_params['sr'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvautovc",
   "language": "python",
   "name": "venvautovc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
