{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  damp_worldWithApers_Size0.25-avgEmbs__svcPitchCond_-bestPerformingSIE_mel80-\n",
      "building synth model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import medfilt\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import random, pdb, os, argparse, shutil, pickle, sys, csv, torch, yaml\n",
    "from torch.backends import cudnn\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.insert(1, '/homes/bdoc3/my_utils')\n",
    "from my_arrays import fix_feat_length, container_to_tensor, tensor_to_array\n",
    "from my_os import recursive_file_retrieval\n",
    "from my_audio.pitch import midi_as_onehot\n",
    "\n",
    "import utils\n",
    "\n",
    "sys.path.insert(1, '/homes/bdoc3/autoSvc')\n",
    "from convert.synthesis import build_model, wavegen\n",
    "\n",
    "\n",
    "# taken from https://gist.github.com/alimanfoo/c5977e87111abe8127453b21204c1065\n",
    "def find_runs(x):\n",
    "    \"\"\"Find runs of consecutive items in an array.\"\"\"\n",
    "\n",
    "    # ensure array\n",
    "    x = np.asanyarray(x)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError('only 1D array supported')\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # handle empty array\n",
    "    if n == 0:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    else:\n",
    "        # find run starts\n",
    "        loc_run_start = np.empty(n, dtype=bool)\n",
    "        loc_run_start[0] = True\n",
    "        np.not_equal(x[:-1], x[1:], out=loc_run_start[1:])\n",
    "        run_starts = np.nonzero(loc_run_start)[0]\n",
    "\n",
    "        # find run values\n",
    "        run_values = x[loc_run_start]\n",
    "\n",
    "        # find run lengths\n",
    "        run_lengths = np.diff(np.append(run_starts, n))\n",
    "\n",
    "        return run_values, run_starts, run_lengths\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-wc', '--which_cuda', type=str, default='train')\n",
    "# parser.add_argument('-rn', '--random_num', type=int, default=None)\n",
    "# config = parser.parse_args()\n",
    "\n",
    "# if config.random_num == None:\n",
    "#     trg_dir = str(random.randint(1000,9999))\n",
    "# else:\n",
    "#     trg_dir = str(config.random_num)\n",
    "trg_dir = str(random.randint(1000,9999))\n",
    "\n",
    "num_model_conds = 4\n",
    "num_convert_conds = 4\n",
    "# this_cuda = config.which_cuda\n",
    "this_cuda = 0\n",
    "voiced_percent_tolerance = 0.6\n",
    "\n",
    "model_names = {\n",
    "               'damp_mel_Size0.25-avgEmbs_with-bestPerformingSIE_mel80-':'M---Sng',\n",
    "               'damp_mel_Size0.25-avgEmbs_EmbLoss__-bestPerformingSIE_mel80-Cont2':'M-E-Sng',\n",
    "               'damp_mel_Size0.25-avgEmbs_withCcLoss-autoVc_pretrainedOnVctk_Mels80-':'M-C-Spk',\n",
    "               'damp_mel_Size0.25-avgEmbs_CcLoss__-bestPerformingSIE_mel80-to500kIters-Cont2':'M-C-Sng'}\n",
    "ckpt = 'ckpt_500000.pt'\n",
    "saved_models_dir = '/homes/bdoc3/my_data/autovc_models/autoSvc'\n",
    "subset = 'test'\n",
    "SVC_data_dir = '/import/c4dm-02/bdoc3/spmel/damp_qianParams/' +subset\n",
    "metadata_root_dir = '/homes/bdoc3/my_data/voice_embs_visuals_metadata'\n",
    "pitch_dir = '/import/c4dm-02/bdoc3/world_data/damp_80_16ms'\n",
    "wavenet_model_params = '/homes/bdoc3/my_data/autovc_models/checkpoint_step001000000_ema.pth'\n",
    "converted_voices_dir = '/homes/bdoc3/my_data/audio_data/output_audio/listeningTest3audio/' +trg_dir\n",
    "device = f'cuda:{this_cuda}'\n",
    "gender_conds = [(i,j) for i in range(2) for j in range(2)]\n",
    "gend_dict = {0:'M', 1:'F'}\n",
    "\n",
    "\n",
    "if not os.path.exists(converted_voices_dir):\n",
    "    os.makedirs(converted_voices_dir)\n",
    "\n",
    "# setup synth model\n",
    "print('building synth model')\n",
    "cudnn.benchmark = True\n",
    "torch.cuda.set_device(device)\n",
    "synth_model = build_model().to(device)\n",
    "checkpoint = torch.load(wavenet_model_params, map_location='cpu')\n",
    "new_state_dict = OrderedDict()\n",
    "for (k, v) in checkpoint[\"state_dict\"].items():\n",
    "    new_state_dict[k] = v.cuda(device)\n",
    "synth_model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoMatchError(Exception):\n",
    "    pass\n",
    "\n",
    "def get_gender_lists():\n",
    "    print('Getting gender info...')\n",
    "    performer_gender_list = utils.get_damp_gender(ignore_unknowns=True)\n",
    "    _, spmel_fps = recursive_file_retrieval(SVC_data_dir)\n",
    "    spmel_perf_id = [os.path.basename(fp).split('_')[0] for fp in spmel_fps]\n",
    "    performance_gender_subset = [perf_gen for perf_gen in performer_gender_list if perf_gen[0] in spmel_perf_id]\n",
    "    females = [perf for perf, gend in performance_gender_subset if gend == ' F']\n",
    "    males = [perf for perf, gend in performance_gender_subset if gend == ' M']\n",
    "    gender_separated_lists = [males, females]\n",
    "    return gender_seperated_lists\n",
    "\n",
    "def get_feats(path):\n",
    "    spec_feats = np.load(path)\n",
    "    fn = os.path.basename(path)\n",
    "    world_feats = np.load(os.path.join(pitch_dir, subset, fn.split('_')[0], fn))\n",
    "    pitches = world_feats[:,-2:]\n",
    "    midi_contour = pitches[:,0]\n",
    "    unvoiced = pitches[:,1].astype(int) == 1\n",
    "    midi_contour[unvoiced] = 0\n",
    "    pitch_feats = midi_contour\n",
    "    pitch_feats = midi_as_onehot(pitch_feats, midi_range)\n",
    "    return spec_feats, pitch_feats\n",
    "\n",
    "\n",
    "def get_song_path(gender):\n",
    "    \n",
    "    gender_list = gender_separated_lists[gender]\n",
    "    rand_int = random.randint(0,len(gender_list)-1)\n",
    "    name = gender_list[rand_int]\n",
    "#     gender_list.pop(rand_int)\n",
    "    song_list = os.listdir(os.path.join(SVC_data_dir, name))\n",
    "    song_name = random.choice(song_list)\n",
    "    song_path = os.path.join(SVC_data_dir, name, song_name)\n",
    "\n",
    "    return song_path, rand_int\n",
    "\n",
    "def get_relevant_avg_pitches(continuous_pitch_feats, window_size, irrelenvant_ind=0):\n",
    "    average_pitches = []\n",
    "    for idx in range(len(continuous_pitch_feats)-window_size):\n",
    "        avging_window = continuous_pitch_feats[idx:idx+window_size]\n",
    "        voiced_window = avging_window!=0\n",
    "\n",
    "        if sum(voiced_window) != irrelenvant_ind:\n",
    "            window_average = round(np.average(avging_window[voiced_window]))\n",
    "        else:\n",
    "            window_average = 0\n",
    "\n",
    "        average_pitches.append(window_average)\n",
    "\n",
    "    average_pitches = np.asarray(average_pitches)\n",
    "    pdb.set_trace()\n",
    "    average_pitches = np.concatenate((average_pitches, np.zeros((window_size))\n",
    "    return average_pitches\n",
    "\n",
    "def matching_pitch_clip(trg_gender, avg_src_pitch, src_path, track_search_tolerance=10, voiced_percent_tolerance=0.7):\n",
    "    \n",
    "    matched_singer_found = False\n",
    "    attempt_num = 0\n",
    "    while matched_singer_found==False:\n",
    "        \n",
    "        trg_path, trg_rand_gend_int = get_song_path(trg_gender)s\n",
    "        if os.path.dirname(trg_path) == os.path.dirname(src_path):\n",
    "            continue\n",
    "    \n",
    "        print(f'attempt num: {attempt_num}, candidate_song: {os.path.basename(trg_path)}')\n",
    "        trg_spec_feats, trg_pitch_feats = get_feats(trg_path)\n",
    "        continuous_pitch_feats = np.argmax(trg_pitch_feats, axis=1)\n",
    "        average_trg_pitches = get_relevant_avg_pitches(continuous_pitch_feats, window_timesteps)\n",
    "        start_of_chunk_idx = best_pitch_matching_idx(average_trg_pitches, avg_src_pitch)\n",
    "        \n",
    "        if start_of_chunk_idx >= 0:\n",
    "            trg_pitch_clip, _ = fix_feat_length(trg_pitch_feats, window_timesteps, offset=start_of_chunk_idx)\n",
    "            voiced = np.argmax(trg_pitch_clip, axis=1) != 0\n",
    "            if (sum(voiced) / len(voiced)) < voiced_percent_tolerance:\n",
    "                continue\n",
    "            matched_singer_found = True\n",
    "            break\n",
    "        \n",
    "        attempt_num += 1\n",
    "        if attempt_num >= track_search_tolerance:\n",
    "            raise NoMatchError(f'No matching pitches after searching {attempt_num} target candidates' )\n",
    "            \n",
    "    trg_spec_clip, _ = fix_feat_length(trg_spec_feats, window_timesteps, offset=start_of_chunk_idx)\n",
    "    \n",
    "    return trg_spec_clip, trg_pitch_clip, trg_rand_gend_int, trg_path\n",
    "\n",
    "\n",
    "# find continuous chunks that are within tolerance of reference pitch, return the index of the random one\n",
    "def best_pitch_matching_idx(average_trg_pitches, ref_pitch, tolerance=2, min_avg_pitch_dur=10):\n",
    "\n",
    "    above_lower = average_trg_pitches > (ref_pitch - tolerance)\n",
    "    below_upper = average_trg_pitches < (ref_pitch + tolerance)\n",
    "    within_range_pitches = above_lower & below_upper\n",
    "#     pdb.set_trace()\n",
    "    eligible_run_indices = []\n",
    "    vals, starts, lengths = find_runs(within_range_pitches)\n",
    "    # for chunks of True in boolean array, if length is long enough, save in list\n",
    "    for i in range(len(vals)):\n",
    "        if vals[i]:\n",
    "            if lengths[i] >= min_avg_pitch_dur:\n",
    "                eligible_run_indices.append(i)\n",
    "    if len(eligible_run_indices) == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        chosen_run_idx = random.choice(eligible_run_indices)\n",
    "        return starts[i]\n",
    "    \n",
    "    \n",
    "def pitch_matched_src_trg(src_gender, trg_gender):\n",
    "\n",
    "    matching_target_found = False\n",
    "    while not matching_target_found:\n",
    "        src_path, src_rand_gend_int = get_song_path(src_gender)\n",
    "    #     src_path = '/import/c4dm-02/bdoc3/spmel/damp_qianParams/test/434587164/434587164_2141814685.npy'\n",
    "\n",
    "        src_spec_feats, src_pitch_feats = get_feats(src_path)\n",
    "\n",
    "        rand_ts = random.randint(0, len(src_spec_feats)-window_timesteps-1)\n",
    "    #     rand_ts = 2981\n",
    "\n",
    "        src_clipped_spec, _ = fix_feat_length(src_spec_feats, window_timesteps, offset=rand_ts)\n",
    "        src_clipped_pitches, _ = fix_feat_length(src_pitch_feats, window_timesteps, offset=rand_ts)\n",
    "        # ensure we do not include avereaging over zero values which represents unvoiced\n",
    "        voiced = np.argmax(src_clipped_pitches, axis=1)!=0\n",
    "        if (sum(voiced) / len(voiced)) < voiced_percent_tolerance:\n",
    "            continue\n",
    "        avg_src_pitch = round(np.average(np.argmax(src_clipped_pitches, axis=1)[voiced]))\n",
    "\n",
    "        print(f'src_song: {os.path.basename(src_path)}, rand_int: {rand_ts}, src_gend: {gend_dict[src_gender]}, avg_src_pitch: {avg_src_pitch}')\n",
    "\n",
    "        print(avg_src_pitch)\n",
    "        try:\n",
    "            spec_pitch_randint = matching_pitch_clip(trg_gender,\n",
    "                                                     avg_src_pitch,\n",
    "                                                     src_path,\n",
    "                                                     voiced_percent_tolerance=voiced_percent_tolerance)\n",
    "\n",
    "            trg_spec_clip, trg_pitch_clip, trg_rand_gend_int, trg_path = spec_pitch_randint\n",
    "            gender_separated_lists[src_gender].pop(src_rand_gend_int)\n",
    "            gender_separated_lists[src_gender].pop(trg_rand_gend_int)\n",
    "            matching_target_found = True\n",
    "        except NoMatchError as e:\n",
    "            continue\n",
    "    \n",
    "    src_data = src_clipped_spec, src_clipped_pitches, src_path\n",
    "    trg_data = trg_spec_clip, trg_pitch_clip, trg_path\n",
    "            \n",
    "    return src_data, trg_data\n",
    "\n",
    "def parse_data(data):\n",
    "    clipped_spec, clipped_pitches, path = data\n",
    "    voice_id = os.path.basename(path).split('_')[0]\n",
    "    sie_emb = subset_metadata[subset_names.index(voice_id)][1]\n",
    "    arr_list = [clipped_spec, clipped_pitches, sie_emb]\n",
    "    tns_list = [container_to_tensor(arr, add_batch_dim=True, device=device) for arr in arr_list]\n",
    "    clipped_spec, clipped_pitches, sie_emb = tns_list\n",
    "    fn = os.path.basename(path)\n",
    "    return clipped_spec, clipped_pitches, sie_emb, fn\n",
    "\n",
    "def get_fn_string(src_fn, trg_fn, rand_ts):\n",
    "    model_str = model_names[model_name]\n",
    "    src_str = gend_dict[src_gender] +src_fn.split('_')[0]\n",
    "    trg_str = gend_dict[trg_gender] +trg_fn.split('_')[0]\n",
    "    return model_str +f'_{src_str}' +f'_timestep{rand_ts}' +f'_{trg_str}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting gender info...\n",
      "src_song: 473926181_1927111247.npy, rand_int: 1526, src_gend: M, avg_src_pitch: 22\n",
      "22\n",
      "attempt num: 0, candidate_song: 792755197_2063165078.npy\n",
      "trg_song: 792755197_2063165078.npy, trg_gend: M, avg_trg_pitch: 23.485148514851485\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHgAAAD7CAYAAAChQFCkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPN0lEQVR4nO2de5BU5ZmHn9/0DAzDRWBAHAFhVIyFrldEIq4morXEmGBqjZd1vWRNkU0wa9REIXvJVrlWTKIxurrJesEQw4agkg1rsbqGmLVSAoIuhqswggg4AiqDAwMM0/3uH30gE5xrn2nO6c/3qZrqPrc+L/XwnXP69Pl9n8wMJ1zKki7AKS4uOHBccOC44MBxwYHjggOnaIIlTZb0hqQ6SdOLtR+nY1SM78GSMsA64BJgC7AUuMbMVvf4zpwOKS/S544H6sxsA4CkOcAUoE3BvdTbKulbpFJ6jubhfTmlejtvbDmasg/2JF3OIRrZ+Z6ZDW1rWbEEDwc2t5reApzbegVJU4GpAJVUca4mFamUnuOtaZ/k5Rsf4sLbp9F/zuKkyznEb+zpTe0tS+wiy8weMbNxZjaugt5JlRE8xRK8FRjZanpENM85whRL8FJgjKRaSb2Aq4H5RdqX0wFFOQebWYukm4HngQww08xWFWNfTscU6yILM1sALCjW5ztdw+9kBY4LDhwXHDguOHBccOC44MBxwYHjggPHBQeOCw4cFxw4LjhwXHDguODAccGB44IDxwUHjgsOHBccOC44cFxw4LjgwHHBgeOCA8cFB07BgiWNlPSipNWSVkm6JZo/WNILktZHr4N6rlynu8RpwS3A7WY2FpgATJM0FpgOLDSzMcDCaNpJiIIFm1m9mb0WvW8E1pAPfk8BZkWrzQIuj1mjE4MeOQdLGg2cCSwBhplZfbToXWBYO9tMlbRM0rID7O+JMopOv01w1ZuT+XBUGfsuG496pz+4HluwpH7AM8A3zOzD1sss38NLm728lGLCf8iji2matIvRn9nI1PueITOkOumSOiWWYEkV5OXONrN50extkmqi5TXA9nglpggzLJulXDkq1JJ0NV0izlW0gMeBNWb2w1aL5gM3RO9vAH5deHnpZE9LLxqzfbA+vVFFr6TL6ZA4LXgicB1wkaTl0d+lwD3AJZLWAxdH0+GQy5K58ygeu+tyBv9sJ+u/f1bSFXVIwQl/M/s9oHYWp79PpBjYspUM3Hcy36x5nutra5Iup0P8TlbguODAccGB44IDxwUHjgsOnKJ1hBYye6eMp+GEcq5//UZalqX711AX3F3KMgy87W1urlnErIvOp2XLmqQr6hA/RBdA2cHfT0pg1DgXHDguOHBccOC44MDxq+juYjk2zT2Bbx9fi30LBq0YRfXji5Kuql28BXcXM45++GXGPNHAM1MeoPlzDUlX1CEuOHBccOC44MBxwYHjggPHBQeOCw4cFxw4fierAJonn0PDiRVMXf3XNL/uP/iHRVmG/jM289XoB/9BW15OuqIO6Yl0YUbS/0l6NpqulbREUp2kX0ajjwbFx+0H/1vIh78P8j3gfjM7EdgJ3NQD+3AKJG58dATwWeCxaFrARcDT0Sqe8E+YuOfgHwF3AP2j6WqgwcwOhme3kO/W4SNImgpMBaikKmYZR4bMibU0nTSEzTv28uDeSQxobky6pE6Jkw++DNhuZq8Wsn0pJvw3XH8MT//7jxjwq370m7KV7I4dSZfUKXFa8ETg81EmuBIYADwADJRUHrXiEcDW+GWmA8vAUWWVlGXB9pdGvyJxetmZYWYjzGw0cDXwWzO7FngRuCJaLciEfylRjDtZdwK3Saojf05+vAj7cLpIj9zoMLPfAb+L3m8AxvfE56aNXg1idmMN+waKgWNPIrv2Tchlky6rQ/xedDcY8fBynvr0WRz1l+9w/pzXKR82NOmSOsVvVXaDXFMTub17qV80gZknDKbPdX0YvOY4Kv/rlaRLaxdvwd3FjFH/tIgx393Lk1+9n503pfu7sAsOnFQIVkVFSXQL2Bpls7y6bxR7dlcmXUqHpELwkE/sZt2DI5Muo1vk6jbxzOXnc/L0dPfUmIqLrEFlWSqrdyVdRrewA81k36hLuoxOSUULdoqHCw4cFxw4LjhwXHDguODAccGB44IDxwUHjgsOHBccOC44cFxw4LjgwHHBgeOCA8cFB44LDpy4+eCBkp6WtFbSGkmflDRY0guS1kev6e7EInDituAHgOfM7GTgdPJJ/+nAQjMbAyyMpp2EiJMPPgq4gChcZmbNZtYATCGf7AdP+CdOnBZcC+wAnog6YXlMUl9gmJnVR+u8Cwxra2NJUyUtk7Rsx/vpDnCVMnEElwNnAT82szOBPRx2ODYzA9rsiqZ1wn9odSZGGU5HxBG8BdhiZkui6afJC98mqQYgek33k+GBEyfh/y6wWdInolmTgNXAfPLJfvCEf+LETTZ8HZgddXa2AfgS+f80cyXdBGwCroy5DycGsQSb2XJgXBuLJsX5XKfn8DtZgeOCA8cFB44LDhwXHDguOHBccOC44MBxwYHjggPHBQeOCw4cFxw4LjhwXHDguODASYXg3Wbs+LDfEd9vWd++lI8aSVlVaYzbVAipEFy/fjDH3/rBEd/v9mtP4yu/Wcj7Xzz9iO/7SJEKwbZvPy1bijO8UmbY0ey6dgIad+pHllXuzPHw2xdR2RDuc9mpEFxM9p5xHM/ecx/rbuz7kWX9nloCF2+lz6/TO+ZCXIIXXLVuBxc88i2OW5A7NK98xHDWP3Qu2792XkkMERuH4AW3bNzEyLtepveCpYfm5Qb35yeTZ9J04W4yAwag8lT0i14UghfcJuve4t7r/4rMin6c9/vt7PpiW0/+hsHHRnD58aNpmXQ2merB5PbtQy+/Tv9NRmO2EuU6375U+dgIXv/lGn7+xAM0XjDm0LyBs1/hDxOr6P/U0g62LG3iJvxvlbRK0kpJv5BUKalW0hJJdZJ+GcVaEmfYshznPXsb/TZ8eGieTTiVN//hdBg3NsHKikucAPhw4O+AcWZ2KpAhP8zs94D7zexEYCdwU08UGpeqeUs46WuvkHt9zaF528b3ZdUND7Hj7OguWlkGpIQqLA5xD9HlQB9J5UAVUA9cRD5KCilP+A9/tp5P3TqNmufeITN0KPXzTmL9v44PSnKc+OhW4F7gbfJidwGvAg3R6N+QzxAPb2v71gn/AyQzmna2biP95i6mZeMmlCnjnGM2Uz16J2WnnUxmaPpHFu0KcQ7Rg8j3x1ELHAv0BSZ3dfvWCf8KehdaRo/Rsm079VOqYF41//Kfs9j05TGdb1QCxDlEXwxsNLMdZnYAmAdMBAZGh2yAEUBxbjL3NGZkt22nfJ9xRq9ysukekrDLxBH8NjBBUpUk8ceE/4vAFdE6nvBPmDjn4CXkL6ZeA1ZEn/UIcCdwm6Q6oJqomyUnGeIm/L8DfOew2RuA8XE+N0nKDhgrmg9gZUb5McPIvvc+1tLS+YYp5WNzJ6urDHhuNTM+dwPNQ7NMXriW3DmnJF1SLFzwYeQaG8mtXAvlOa4asJpcZWn34eWCA8cFB44LDhwXHDguOHBccOC44MBxwYHjggPHBbdDpqGc5/aMYu+QCspHH5d0OQXjgtvhpLvfYM4XPs0Jt62hz5NNJZtADPeR/phkd+6krKWF2qoPyFoZr2pA0iUVhLfgwPEW3AG2dy8LHriA3ceJA482MfC3fah+bFHSZXULb8EdYC0tDJ65iGNf2s/iP3+Y98eVXo7YBQeOCw4cFxw4fpHVAaroxeZvjmPf0TkmzL2dYxeXXm8ALrgDVNmbK67+3/z34IkDyO3Zk3RJ3cYP0YHjggPHBQdOp4IlzZS0XdLKVvMGS3pB0vrodVA0X5IejNL9f5B0VjGLdzqnKy34p3w0FjodWGhmY4CF/HFg6M8AY6K/qcCPe6ZMp1A6FWxmLwGHdyQ5hXx6H/40xT8F+JnlWUw+SlrTQ7U6BVDoOXiYmdVH798FhkXvhwObW62X6oT/x4HYF1lmZkC37wCkLeEfKoUK3nbw0Bu9bo/mbwVGtlqvdBL+gVKo4Pnk0/vwpyn++cD10dX0BGBXq0O5kwCd3qqU9AvgU8AQSVvIB77vAeZKugnYBFwZrb4AuBSoA5qALxWhZqcbdCrYzK5pZ9GkNtY1YFrcopyew+9kBY4LDhwXHDguOHBccOC44MBxwYHjggPHBQeOCw4cFxw4LjhwXHDguODAccGB44IDx8Nn7aCKXpT1rWJ3yx725yqSLqdgXHA7vPWPZzP5s0t56eFzqV6xG2ta2flGKcQFH0ZmSDXNfzaK/UOz5Ex5uUtXJF1Wwfg5+DAaJo3hyVkP0ntHhvUX9sKWlWbLPYi34MOwDBydqQJTSQa+D8dbcGskTNBC6XWX1B7egiMyQ6p56yc1NO/fz4V3fJ3jl78fhGYXHKHKSu46bT5PvDORA//xLlkrvQ5X2sIP0YFTaML/B5LWRin+X0ka2GrZjCjh/4akvyhS3U4XKTTh/wJwqpmdBqwDZgBIGgtcDZwSbfNvkkp7bLgSp6CEv5n9T6th3BeTj4lCPuE/x8z2m9lG8iG0khiJNPfBTr77g2upe2k0b889lcarJiRdUo/QE+fgvwH+O3pfsgn/XFMT1Y8uYtBaY+V5s/hgbBiXJ7H+FZL+HmgBZnd3W0/4HxkK/pok6UbgMmBSFBsFT/injoJasKTJwB3A582sqdWi+cDVknpLqiXfndIr8ct0CqXQhP8MoDfwgiSAxWb2t2a2StJcYDX5Q/c0MwvhhlDJUmjC//EO1r8buDtOUU7PEcalotMuLjhwXHDguODAccGB44IDxwUHjiwFTy5I2gHsAd5LupYuMoR01TrKzIa2tSAVggEkLTOzcUnX0RVKqVY/RAeOCw6cNAl+JOkCukHJ1Jqac7BTHNLUgp0i4IIDJxWCJU2OnqOukzS98y2ODJJGSnpR0mpJqyTdEs3/Z0lbJS2P/i5Nutb2SPwcHD03vQ64hPxTmEuBa8xsdaKFcWhEmRoze01Sf+BV8oOAXQnsNrN7k6yvK6ShBY8H6sxsg5k1A3PIP1+dOGZWb2avRe8bgTW08xhwWkmD4C4/S50kkkYDZwJLolk3R9GdmQcH50wjaRCceiT1A54BvmFmH5IfdPME4AygHrgvueo6Jg2CU/0staQK8nJnm9k8ADPbZmZZM8sBj5LieE4aBC8FxkiqldSLfHhtfsI1AfnxkMk/QbrGzH7Yan7rEVW/AKS2I4/EA+Bm1iLpZuB5IAPMNLNVCZd1kInAdcAKScujed8GrpF0BvlBOd8CvpJEcV0h8a9JTnFJwyHaKSIuOHBccOC44MBxwYHjggPHBQfO/wOhVJOaek8sPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for j, model_name in enumerate(model_names.keys()):\n",
    "\n",
    "    # model_name = 'damp_mel_Size0.25-avgEmbs_with-bestPerformingSIE_mel80-'\n",
    "\n",
    "    # load up models\n",
    "    this_svc_model_dir = os.path.join(saved_models_dir, model_name)\n",
    "    ckpt_path = os.path.join(this_svc_model_dir, ckpt)\n",
    "    # if j != 0:\n",
    "    #     del sys.path[1]\n",
    "    sys.path.insert(1, this_svc_model_dir)\n",
    "    from this_train_params import *\n",
    "    if not 'pkl_fn_extras' in globals():\n",
    "        pkl_fn_extras = ''\n",
    "\n",
    "    with open(os.path.join(SVC_feat_dir, 'feat_params.yaml')) as File:\n",
    "        SVC_feat_params = yaml.load(File, Loader=yaml.FullLoader)\n",
    "    with open(os.path.join(SIE_feat_dir, 'feat_params.yaml')) as File:\n",
    "        SIE_feat_params = yaml.load(File, Loader=yaml.FullLoader)\n",
    "\n",
    "    if SVC_pitch_cond:\n",
    "        pitch_dim = len(midi_range)+1\n",
    "    else:\n",
    "        pitch_dim = 0\n",
    "\n",
    "    print(f'Loading model: {model_name}')\n",
    "    G, _, _ = utils.setup_gen(dim_neck, dim_emb, dim_pre, sample_freq, 80, pitch_dim, device, ckpt_path, adam_init)\n",
    "\n",
    "    sie_model_name = os.path.basename(SIE_model_path)\n",
    "    SIE_dataset_name = os.path.basename(SIE_feat_dir)\n",
    "    metadata_path = os.path.join(metadata_root_dir,\n",
    "                                 sie_model_name,\n",
    "                                 SIE_dataset_name,\n",
    "                                 subset,\n",
    "                                 f'voices_metadata{pkl_fn_extras}.pkl')\n",
    "    subset_metadata = pickle.load(open(metadata_path, \"rb\"))\n",
    "    subset_names = [metad[0] for metad in subset_metadata]\n",
    "\n",
    "    # if model_name == 'damp_mel_Size0.25-avgEmbs_withCcLoss-autoVc_pretrainedOnVctk_Mels80-':\n",
    "    #     temp_gender_conds = gender_conds[2:]\n",
    "    # else:\n",
    "    #     temp_gender_conds = gender_conds\n",
    "\n",
    "    temp_gender_conds = gender_conds\n",
    "\n",
    "    # GET GENDER LIST OF DAMP SINGERS AND USE ONLY TEST SUBSET ENTRIES\n",
    "\n",
    "    for i, (src_gender, trg_gender) in enumerate(temp_gender_conds):\n",
    "\n",
    "    #     src_gender, trg_gender = (0, 0)\n",
    "        print(f'Getting feats for condition: {i+1}')\n",
    "        src_data, trg_data = pitch_matched_src_trg(src_gender, trg_gender, voiced_percent_tolerance)\n",
    "        src_clipped_spec, src_clipped_pitches, src_sie_emb, src_fn = parse_data(src_data)\n",
    "        trg_clipped_spec, trg_clipped_pitches, trg_sie_emb, trg_fn = parse_data(trg_data)\n",
    "\n",
    "        if SVC_pitch_cond:\n",
    "            src_pitch_tns = None\n",
    "\n",
    "        # conversion\n",
    "        _, converted_feats, _, _, _ = G(src_clipped_spec_tns, src_emb_tns, trg_emb_tns, src_pitch_tns)\n",
    "        converted_feats = tensor_to_array(converted_feats)\n",
    "        converted_name = get_fn_string(src_fn, trg_fn, rand_ts)\n",
    "\n",
    "        # synthesis\n",
    "        dst_dir = os.path.join(converted_voices_dir, converted_name) +'.wav'\n",
    "        if os.path.exists(dst_dir):\n",
    "            continue\n",
    "        print(f'Synthesizing converted audio at: {dst_dir}')\n",
    "        waveform = wavegen(synth_model, this_cuda, c=converted_feats)\n",
    "        sf.write(dst_dir, waveform, samplerate=SVC_feat_params['sr'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
